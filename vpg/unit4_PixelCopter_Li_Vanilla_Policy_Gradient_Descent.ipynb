{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vLmAuU-Ep_aj"
   },
   "outputs": [],
   "source": [
    "# 1. create a Policy Network & Value Network\n",
    "# Policy: MLP with (State -> Act)\n",
    "# Value: MLP with (State -> 1(value))\n",
    "\n",
    "# 2. VPGD algorithm taking Policy, Value, Envornment as input\n",
    "\n",
    "\n",
    "# 3. plot loss\n",
    "\n",
    "# *4. update to hugging face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zuCgHOA-pn1b",
    "outputId": "89e604e5-96d3-4fa6-ca65-ff557e222dd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ntasfi/PyGame-Learning-Environment.git (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1))\n",
      "  Cloning https://github.com/ntasfi/PyGame-Learning-Environment.git to /private/var/folders/_t/z3tjm9014td81ypk6h_0423c0000gn/T/pip-req-build-1arjw3cc\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/ntasfi/PyGame-Learning-Environment.git /private/var/folders/_t/z3tjm9014td81ypk6h_0423c0000gn/T/pip-req-build-1arjw3cc\n",
      "  Resolved https://github.com/ntasfi/PyGame-Learning-Environment.git to commit 3dbe79dc0c35559bb441b9359948aabf9bb3d331\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting git+https://github.com/simoninithomas/gym-games (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2))\n",
      "  Cloning https://github.com/simoninithomas/gym-games to /private/var/folders/_t/z3tjm9014td81ypk6h_0423c0000gn/T/pip-req-build-qpdd_naz\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/simoninithomas/gym-games /private/var/folders/_t/z3tjm9014td81ypk6h_0423c0000gn/T/pip-req-build-qpdd_naz\n",
      "  Resolved https://github.com/simoninithomas/gym-games to commit f31695e4ba028400628dc054ee8a436f28193f0b\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: huggingface_hub in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (0.26.2)\n",
      "Requirement already satisfied: imageio-ffmpeg in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 4)) (0.5.1)\n",
      "Requirement already satisfied: pyyaml==6.0 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 5)) (6.0)\n",
      "Requirement already satisfied: numpy in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from ple==0.0.1->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from ple==0.0.1->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (11.0.0)\n",
      "Requirement already satisfied: gym>=0.13.0 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (0.26.2)\n",
      "Requirement already satisfied: setuptools>=65.5.1 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (75.1.0)\n",
      "Requirement already satisfied: pygame>=1.9.6 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: filelock in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (24.1)\n",
      "Requirement already satisfied: requests in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (4.11.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (7.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2024.8.30)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (3.20.2)\n",
      "Requirement already satisfied: pygame in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (2.6.1)\n",
      "Requirement already satisfied: ple in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (0.0.1)\n",
      "Requirement already satisfied: numpy in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from ple) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from ple) (11.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt\n",
    "!pip install pygame ple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhCtkGhRVkny",
    "outputId": "8ca6339f-026e-4914-b27b-4fbbc1149a1c"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, snapshot_download\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "import imageio\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import os\n",
    "\n",
    "def record_video(env, policy, out_directory, fps=30):\n",
    "    \"\"\"\n",
    "    Generate a replay video of the agent\n",
    "    :param env\n",
    "    :param Qtable: Qtable of our agent\n",
    "    :param out_directory\n",
    "    :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    images.append(img)\n",
    "    while not done:\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = take_action(policy, torch.as_tensor(state))\n",
    "        state, reward, done, info = env.step(action)  # We directly put next_state = state for recording logic\n",
    "        img = env.render(mode=\"rgb_array\")\n",
    "        images.append(img)\n",
    "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n",
    "    \n",
    "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
    "    \"\"\"\n",
    "    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "    :param env: The evaluation environment\n",
    "    :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "    :param policy: The Reinforce agent\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action, _ = policy.act(state)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "def push_to_hub(repo_id,\n",
    "                model,\n",
    "                hyperparameters,\n",
    "                eval_env,\n",
    "                video_fps=30\n",
    "                ):\n",
    "  \"\"\"\n",
    "  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n",
    "  This method does the complete pipeline:\n",
    "  - It evaluates the model\n",
    "  - It generates the model card\n",
    "  - It generates a replay video of the agent\n",
    "  - It pushes everything to the Hub\n",
    "\n",
    "  :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n",
    "  :param model: the pytorch model we want to save\n",
    "  :param hyperparameters: training hyperparameters\n",
    "  :param eval_env: evaluation environment\n",
    "  :param video_fps: how many frame per seconds to record our video replay\n",
    "  \"\"\"\n",
    "\n",
    "  _, repo_name = repo_id.split(\"/\")\n",
    "  api = HfApi()\n",
    "\n",
    "  # Step 1: Create the repo\n",
    "  repo_url = api.create_repo(\n",
    "        repo_id=repo_id,\n",
    "        exist_ok=True,\n",
    "  )\n",
    "\n",
    "  with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    local_directory = Path(tmpdirname)\n",
    "\n",
    "    # Step 2: Save the model\n",
    "    torch.save(model, local_directory / \"model.pt\")\n",
    "\n",
    "    # Step 3: Save the hyperparameters to JSON\n",
    "    with open(local_directory / \"hyperparameters.json\", \"w\") as outfile:\n",
    "      json.dump(hyperparameters, outfile)\n",
    "\n",
    "    # Step 4: Evaluate the model and build JSON\n",
    "    mean_reward, std_reward = evaluate_agent(eval_env,\n",
    "                                            hyperparameters[\"max_t\"],\n",
    "                                            hyperparameters[\"n_evaluation_episodes\"],\n",
    "                                            model)\n",
    "    # Get datetime\n",
    "    eval_datetime = datetime.datetime.now()\n",
    "    eval_form_datetime = eval_datetime.isoformat()\n",
    "\n",
    "    evaluate_data = {\n",
    "          \"env_id\": hyperparameters[\"env_id\"],\n",
    "          \"mean_reward\": mean_reward,\n",
    "          \"n_evaluation_episodes\": hyperparameters[\"n_evaluation_episodes\"],\n",
    "          \"eval_datetime\": eval_form_datetime,\n",
    "    }\n",
    "\n",
    "    # Write a JSON file\n",
    "    with open(local_directory / \"results.json\", \"w\") as outfile:\n",
    "        json.dump(evaluate_data, outfile)\n",
    "\n",
    "    # Step 5: Create the model card\n",
    "    env_name = hyperparameters[\"env_id\"]\n",
    "\n",
    "    metadata = {}\n",
    "    metadata[\"tags\"] = [\n",
    "          env_name,\n",
    "          \"reinforce\",\n",
    "          \"reinforcement-learning\",\n",
    "          \"custom-implementation\",\n",
    "          \"deep-rl-class\"\n",
    "      ]\n",
    "\n",
    "    # Add metrics\n",
    "    eval = metadata_eval_result(\n",
    "        model_pretty_name=repo_name,\n",
    "        task_pretty_name=\"reinforcement-learning\",\n",
    "        task_id=\"reinforcement-learning\",\n",
    "        metrics_pretty_name=\"mean_reward\",\n",
    "        metrics_id=\"mean_reward\",\n",
    "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
    "        dataset_pretty_name=env_name,\n",
    "        dataset_id=env_name,\n",
    "      )\n",
    "\n",
    "    # Merges both dictionaries\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    model_card = f\"\"\"\n",
    "  # **Reinforce** Agent playing **{env_id}**\n",
    "  This is a trained model of a **Reinforce** agent playing **{env_id}** .\n",
    "  To learn to use this model and train yours check Unit 4 of the Deep Reinforcement Learning Course: https://huggingface.co/deep-rl-course/unit4/introduction\n",
    "  \"\"\"\n",
    "\n",
    "    readme_path = local_directory / \"README.md\"\n",
    "    readme = \"\"\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
    "          readme = f.read()\n",
    "    else:\n",
    "      readme = model_card\n",
    "\n",
    "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "      f.write(readme)\n",
    "\n",
    "    # Save our metrics to Readme metadata\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "    # Step 6: Record a video\n",
    "    video_path =  local_directory / \"replay.mp4\"\n",
    "    record_video(env, model, video_path, video_fps)\n",
    "\n",
    "    # Step 7. Push everything to the Hub\n",
    "    api.upload_folder(\n",
    "          repo_id=repo_id,\n",
    "          folder_path=local_directory,\n",
    "          path_in_repo=\".\",\n",
    "    )\n",
    "\n",
    "    print(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ByztVnRLPc7"
   },
   "source": [
    "# Add Entropy regularization\n",
    "$\n",
    "\\begin{align*}\n",
    "J(\\theta) &= E_{\\tau \\sim \\pi}[\\sum_{t=0}^{T} log\\pi(at|st) R + \\beta H(\\pi(.|st) \\\\\n",
    "&= E_{st, at}[log\\pi(at|st).R + \\beta H(\\pi(.|st)]\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJMhj_5cLNVk"
   },
   "source": [
    "# TODO\n",
    "1. check performance\n",
    "2. check execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "DkNCmK5cBdIr",
    "outputId": "3c16fc4a-e53e-42ec-a901-8befda01a0c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from model.pth...\n",
      "0th iteration, loss=-0.0046191090345243805, rewards=45.52, std=45.94071832263836 lens=297.81\n",
      "\n",
      "1th iteration, loss=-0.0030166849214648813, rewards=46.85, std=46.68455311984897 lens=305.38\n",
      "\n",
      "2th iteration, loss=0.0033260298684153785, rewards=42.16, std=37.91034687258876 lens=279.16\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 249\u001b[0m\n\u001b[1;32m    246\u001b[0m     video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplay-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformatted_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m     record_video(env, policy, video_path, video_fps)\n\u001b[0;32m--> 249\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 224\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-2\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epoch):\n\u001b[0;32m--> 224\u001b[0m     loss, reward, lens, reward_st \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# if i % 10 == 0:\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m#     print(f\"{i}th iteration, loss={loss}, rewards={reward}, std={reward_st} lens={lens}\\n\")\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mth iteration, loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, rewards=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, std=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward_st\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m lens=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[34], line 157\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(env, policy, num_batch, optimizer, iter)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    156\u001b[0m     action \u001b[38;5;241m=\u001b[39m take_action(policy, torch\u001b[38;5;241m.\u001b[39mtensor(st))\n\u001b[0;32m--> 157\u001b[0m     st_1, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     batch_obs\u001b[38;5;241m.\u001b[39mappend(st\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m    159\u001b[0m     cur_acts\u001b[38;5;241m.\u001b[39mappend(action)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gym_pygame/envs/base.py:44\u001b[0m, in \u001b[0;36mBaseEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 44\u001b[0m   reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgameOb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_set\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m   done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgameOb\u001b[38;5;241m.\u001b[39mgame_over()\n\u001b[1;32m     46\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgameOb\u001b[38;5;241m.\u001b[39mgetGameState(), reward, done, {})\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ple/ple.py:376\u001b[0m, in \u001b[0;36mPLE.act\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Perform an action on the game. We lockstep frames with actions. If act is not called the game will not run.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m \n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oneStepAct\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_skip\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ple/ple.py:376\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Perform an action on the game. We lockstep frames with actions. If act is not called the game will not run.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m \n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oneStepAct\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_skip))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ple/ple.py:398\u001b[0m, in \u001b[0;36mPLE._oneStepAct\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_steps):\n\u001b[1;32m    397\u001b[0m     time_elapsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tick()\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_elapsed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_draw_frame()\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_steps\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ple/games/pixelcopter.py:283\u001b[0m, in \u001b[0;36mPixelcopter.step\u001b[0;34m(self, dt)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayer\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_climbing, dt)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_group\u001b[38;5;241m.\u001b[39mupdate(dt)\n\u001b[0;32m--> 283\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterrain_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m hits \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39msprite\u001b[38;5;241m.\u001b[39mspritecollide(\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_group, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m creep \u001b[38;5;129;01min\u001b[39;00m hits:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pygame/sprite.py:556\u001b[0m, in \u001b[0;36mAbstractGroup.update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"call the update method of every member sprite\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03mGroup.update(*args, **kwargs): return None\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    553\u001b[0m \n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sprite \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msprites():\n\u001b[0;32m--> 556\u001b[0m     \u001b[43msprite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ple/games/pixelcopter.py:121\u001b[0m, in \u001b[0;36mTerrain.update\u001b[0;34m(self, dt)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, dt):\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m*\u001b[39m dt\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrect\u001b[38;5;241m.\u001b[39mcenter \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos\u001b[38;5;241m.\u001b[39mx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos\u001b[38;5;241m.\u001b[39my)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# vanila policy gradient descent with reward to go and BASELINE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import gym_pygame\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# hyperparamebers to tune\n",
    "learning_rate =  1e-4\n",
    "# beta = 1e-4\n",
    "# decay = 0.99\n",
    "gamma = 0.9\n",
    "\n",
    "debug = False\n",
    "\n",
    "# File path to the model\n",
    "model_file_path = \"model.pth\"\n",
    "video_fps = 30\n",
    "# video_path = 'replay.mp4'\n",
    "\n",
    "\n",
    "def load_model(model, model_file_path):\n",
    "  # Check if the model file exists\n",
    "    if os.path.exists(model_file_path):\n",
    "        print(f\"Loading model from {model_file_path}...\")\n",
    "        # Load the saved model state\n",
    "        model.load_state_dict(torch.load(model_file_path))\n",
    "        model.eval()  # Set the model to evaluation mode if not training further\n",
    "    else:\n",
    "        print(f\"No saved model found at {model_file_path}. Creating a new model...\")\n",
    "        \n",
    "def save_model(model, model_file_path):\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), model_file_path)\n",
    "\n",
    "def disable_random():\n",
    "    # disable all random ness\n",
    "    # Set the random seed for Python's random module\n",
    "    random.seed(0)\n",
    "    # Set the random seed for NumPy (if you're using it)\n",
    "    np.random.seed(0)\n",
    "    # Set the random seed for PyTorch (CPU and GPU)\n",
    "    torch.manual_seed(0)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(0)\n",
    "\n",
    "def print_actions(batch_abs):\n",
    "    #debug if action is random enough\n",
    "    if debug:\n",
    "        print(f'actions: {batch_abs}')\n",
    "\n",
    "def print_rewards(traj_rewards):\n",
    "    #debug if the random actions actually yield difference in rewards\n",
    "    if debug:\n",
    "        print(f'traj rewards: {traj_rewards}')\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    # (TODO) MLP/Policy can be reconfigured to contain multiple layers\n",
    "    def __init__(self, dim_input, dim_hidden, dim_output):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim_input, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, 2 * dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * dim_hidden, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, dim_output)\n",
    "            # nn.Linear(dim_hidden, dim_output)\n",
    "\n",
    "        )\n",
    "    # return logits of size(dim_output)\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "# action(st), H(st) -> at based on probability distribution\n",
    "def take_action(policy, s):\n",
    "    logits = policy(s)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    categorical_dist = torch.distributions.Categorical(probs = probs)\n",
    "    action = categorical_dist.sample()\n",
    "    return action.item()\n",
    "\n",
    "def get_logp(policy, obs, acts):\n",
    "    logits = policy(obs)\n",
    "    categorical_dists = torch.distributions.Categorical(logits = logits)\n",
    "    return categorical_dists.log_prob(acts)\n",
    "\n",
    "def get_probs(policy, obs):\n",
    "    logits = policy(obs)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    return probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_loss(policy, batch_obs, batch_acts, batch_rewards):\n",
    "    # encourage large entropy at the begining\n",
    "    logps = get_logp(policy, batch_obs, batch_acts)\n",
    "    # probs = get_probs(policy, batch_obs)\n",
    "    # entropy = -torch.sum((probs * torch.log(probs))).mean()\n",
    "    return -(logps * batch_rewards).mean()\n",
    "\n",
    "# list of [float]\n",
    "def compute_reward_to_go(rewards):\n",
    "    n = len(rewards)\n",
    "    rtgs = [0] * n\n",
    "    for i in reversed(range(n)):\n",
    "        rtgs[i] = rewards[i] +  gamma * (rtgs[i + 1] if i + 1 < n else 0)\n",
    "    return rtgs\n",
    "\n",
    "# extract baseline based on mean of rewards at step t\n",
    "def distract_baseline(rewards):\n",
    "    s = {}\n",
    "    eps = 1e-10\n",
    "    for traj_reward in rewards:\n",
    "        for i in range(len(traj_reward)):\n",
    "            s[i] = s.get(i, []) + [traj_reward[i]]\n",
    "    for traj_reward in rewards:\n",
    "        for i in range(len(traj_reward)):\n",
    "            traj_reward[i] = (traj_reward[i] - np.mean(s[i])) / (np.std(s[i]) + eps)\n",
    "    # if debug:\n",
    "    #     print('- baseline rewards')\n",
    "    #     print(rewards)\n",
    "    rewards = np.concatenate(rewards)\n",
    "    return rewards\n",
    "\n",
    "# return loss, avg_reward, avg_lens for this epoch\n",
    "def train_epoch(env, policy, num_batch, optimizer, iter):\n",
    "    # collecting trajectories\n",
    "    batch_obs = []\n",
    "    batch_acts = []\n",
    "    batch_rewards = [] # reward to go\n",
    "    batch_lens = []\n",
    "    batch_traj_rewards = [] # keep track of total return for each trajectory\n",
    "    rtgs = [] # rewards to go array\n",
    "    debug_abs = []\n",
    "    debug_traj_rewards = []\n",
    "\n",
    "    # here batch is the total logp * R(tau) items, not the number of trajectories\n",
    "    start = timer()\n",
    "    for i in range(num_batch):\n",
    "        st = env.reset()\n",
    "        cur_obs = []\n",
    "        cur_acts = []\n",
    "        cur_rewards = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = take_action(policy, torch.tensor(st))\n",
    "            st_1, reward, done, _ = env.step(action)\n",
    "            batch_obs.append(st.copy())\n",
    "            cur_acts.append(action)\n",
    "            batch_acts.append(action)\n",
    "            cur_rewards.append(reward)\n",
    "            # batch_entropy.append(probs)\n",
    "            # cur_rewards.append(1)\n",
    "            st = st_1\n",
    "        debug_abs.append(cur_acts)\n",
    "        R = sum(cur_rewards)\n",
    "        debug_traj_rewards.append(R)\n",
    "        # reward to go - baseline\n",
    "        batch_traj_rewards.append(R)\n",
    "        cur_rewards=compute_reward_to_go(cur_rewards)\n",
    "        if debug:\n",
    "            print('actions---')\n",
    "            print(cur_acts)\n",
    "            print('debug actual rewards to go')\n",
    "            print_rewards(cur_rewards)\n",
    "\n",
    "        rtgs.append(cur_rewards)\n",
    "        batch_lens.append(len(cur_rewards))\n",
    "    if debug:\n",
    "        t1 = timer()\n",
    "        print(f\"time for collecting trajector {t1 - start}\")\n",
    "    rtgs = distract_baseline(rtgs) # 2d array convert into 1d array)\n",
    "\n",
    "\n",
    "    # print(f'time for collecting trajectories is {t1  - start}')\n",
    "\n",
    "    # reinforce\n",
    "    # Use a function which has similar effect of gradient of J(\\theta) to approximate finding\n",
    "    # maxium value of J(\\theta). using the approximate loss function mainly due to J(\\theta)\n",
    "    # contains uncertain environmental variable in MDP and cannot be deffeniated directly.\n",
    "    # E(sigma(logpP(at|st)) * R(tau)) to approximate J(\\theta)\n",
    "    # debug timestamp\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # print(batch_logp * batch_rewards)\n",
    "    loss = compute_loss(\n",
    "        policy,\n",
    "        batch_obs = torch.as_tensor(batch_obs),\n",
    "        batch_acts = torch.as_tensor(batch_acts),\n",
    "        batch_rewards = torch.as_tensor(rtgs)\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if debug:\n",
    "        print(f\"time for training: {timer() - t1}\")\n",
    "    return loss, np.mean(batch_traj_rewards), np.mean(batch_lens), np.std(batch_traj_rewards)\n",
    "def train():\n",
    "    start = timer()\n",
    "    #debug\n",
    "    # disable_random()\n",
    "    env = gym.make(\"Pixelcopter-PLE-v0\")\n",
    "    # make the action deterministic\n",
    "    dim_input = env.observation_space.shape[0]\n",
    "    dim_output = env.action_space.n\n",
    "    policy = Policy(dim_input, 32, dim_output).double()\n",
    "    load_model(policy, model_file_path)\n",
    "    optimizer = torch.optim.Adam(policy.parameters(), lr = learning_rate)\n",
    "\n",
    "    num_epoch = 10\n",
    "    num_batch = 200\n",
    "    rewards = []\n",
    "    beta = 1e-2\n",
    "    for i in range(num_epoch):\n",
    "        loss, reward, lens, reward_st = train_epoch(env, policy, num_batch, optimizer, i)\n",
    "        # if i % 10 == 0:\n",
    "        #     print(f\"{i}th iteration, loss={loss}, rewards={reward}, std={reward_st} lens={lens}\\n\")\n",
    "        print(f\"{i}th iteration, loss={loss}, rewards={reward}, std={reward_st} lens={lens}\\n\")\n",
    "\n",
    "        rewards.append(reward)\n",
    "    print(f'total time for traning: {timer() - start}')\n",
    "    plt.plot(list(range(num_epoch)), rewards, marker='o', linestyle='-', label='Rewards per Epoch')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.title('Rewards per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    save_model(policy, model_file_path)\n",
    "    policy.eval()\n",
    "    current_time = datetime.now()\n",
    "\n",
    "    # Format the date and time as mm-dd-tt\n",
    "    formatted_time = current_time.strftime(\"%m-%d-%H:%M\")  # %H%M represents hours and minutes (24-hour format)\n",
    "\n",
    "    # Append the formatted time to the string 'replay'\n",
    "    video_path = f\"replay-{formatted_time}.mp4\"\n",
    "    record_video(env, policy, video_path, video_fps)\n",
    "\n",
    "train()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Conda Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
