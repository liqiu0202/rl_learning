{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vLmAuU-Ep_aj"
   },
   "outputs": [],
   "source": [
    "# 1. create a Policy Network & Value Network\n",
    "# Policy: MLP with (State -> Act)\n",
    "# Value: MLP with (State -> 1(value))\n",
    "\n",
    "# 2. VPGD algorithm taking Policy, Value, Envornment as input\n",
    "\n",
    "\n",
    "# 3. plot loss\n",
    "\n",
    "# *4. update to hugging face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zuCgHOA-pn1b",
    "outputId": "89e604e5-96d3-4fa6-ca65-ff557e222dd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ntasfi/PyGame-Learning-Environment.git (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1))\n",
      "  Cloning https://github.com/ntasfi/PyGame-Learning-Environment.git to /private/var/folders/_t/z3tjm9014td81ypk6h_0423c0000gn/T/pip-req-build-1arjw3cc\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/ntasfi/PyGame-Learning-Environment.git /private/var/folders/_t/z3tjm9014td81ypk6h_0423c0000gn/T/pip-req-build-1arjw3cc\n",
      "  Resolved https://github.com/ntasfi/PyGame-Learning-Environment.git to commit 3dbe79dc0c35559bb441b9359948aabf9bb3d331\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting git+https://github.com/simoninithomas/gym-games (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2))\n",
      "  Cloning https://github.com/simoninithomas/gym-games to /private/var/folders/_t/z3tjm9014td81ypk6h_0423c0000gn/T/pip-req-build-qpdd_naz\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/simoninithomas/gym-games /private/var/folders/_t/z3tjm9014td81ypk6h_0423c0000gn/T/pip-req-build-qpdd_naz\n",
      "  Resolved https://github.com/simoninithomas/gym-games to commit f31695e4ba028400628dc054ee8a436f28193f0b\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: huggingface_hub in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (0.26.2)\n",
      "Requirement already satisfied: imageio-ffmpeg in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 4)) (0.5.1)\n",
      "Requirement already satisfied: pyyaml==6.0 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 5)) (6.0)\n",
      "Requirement already satisfied: numpy in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from ple==0.0.1->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from ple==0.0.1->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (11.0.0)\n",
      "Requirement already satisfied: gym>=0.13.0 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (0.26.2)\n",
      "Requirement already satisfied: setuptools>=65.5.1 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (75.1.0)\n",
      "Requirement already satisfied: pygame>=1.9.6 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: filelock in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (24.1)\n",
      "Requirement already satisfied: requests in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (4.11.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (7.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2024.8.30)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (3.20.2)\n",
      "Requirement already satisfied: pygame in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (2.6.1)\n",
      "Requirement already satisfied: ple in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (0.0.1)\n",
      "Requirement already satisfied: numpy in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from ple) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /Users/liqiu/opt/anaconda3/lib/python3.9/site-packages (from ple) (11.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt\n",
    "!pip install pygame ple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhCtkGhRVkny",
    "outputId": "8ca6339f-026e-4914-b27b-4fbbc1149a1c"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, snapshot_download\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "import imageio\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import os\n",
    "\n",
    "def record_video(env, policy, out_directory, fps=30):\n",
    "    \"\"\"\n",
    "    Generate a replay video of the agent\n",
    "    :param env\n",
    "    :param Qtable: Qtable of our agent\n",
    "    :param out_directory\n",
    "    :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    images.append(img)\n",
    "    while not done:\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = take_action(policy, torch.as_tensor(state))\n",
    "        state, reward, done, info = env.step(action)  # We directly put next_state = state for recording logic\n",
    "        img = env.render(mode=\"rgb_array\")\n",
    "        images.append(img)\n",
    "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n",
    "    \n",
    "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
    "    \"\"\"\n",
    "    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "    :param env: The evaluation environment\n",
    "    :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "    :param policy: The Reinforce agent\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action, _ = policy.act(state)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "def push_to_hub(repo_id,\n",
    "                model,\n",
    "                hyperparameters,\n",
    "                eval_env,\n",
    "                video_fps=30\n",
    "                ):\n",
    "  \"\"\"\n",
    "  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n",
    "  This method does the complete pipeline:\n",
    "  - It evaluates the model\n",
    "  - It generates the model card\n",
    "  - It generates a replay video of the agent\n",
    "  - It pushes everything to the Hub\n",
    "\n",
    "  :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n",
    "  :param model: the pytorch model we want to save\n",
    "  :param hyperparameters: training hyperparameters\n",
    "  :param eval_env: evaluation environment\n",
    "  :param video_fps: how many frame per seconds to record our video replay\n",
    "  \"\"\"\n",
    "\n",
    "  _, repo_name = repo_id.split(\"/\")\n",
    "  api = HfApi()\n",
    "\n",
    "  # Step 1: Create the repo\n",
    "  repo_url = api.create_repo(\n",
    "        repo_id=repo_id,\n",
    "        exist_ok=True,\n",
    "  )\n",
    "\n",
    "  with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    local_directory = Path(tmpdirname)\n",
    "\n",
    "    # Step 2: Save the model\n",
    "    torch.save(model, local_directory / \"model.pt\")\n",
    "\n",
    "    # Step 3: Save the hyperparameters to JSON\n",
    "    with open(local_directory / \"hyperparameters.json\", \"w\") as outfile:\n",
    "      json.dump(hyperparameters, outfile)\n",
    "\n",
    "    # Step 4: Evaluate the model and build JSON\n",
    "    mean_reward, std_reward = evaluate_agent(eval_env,\n",
    "                                            hyperparameters[\"max_t\"],\n",
    "                                            hyperparameters[\"n_evaluation_episodes\"],\n",
    "                                            model)\n",
    "    # Get datetime\n",
    "    eval_datetime = datetime.datetime.now()\n",
    "    eval_form_datetime = eval_datetime.isoformat()\n",
    "\n",
    "    evaluate_data = {\n",
    "          \"env_id\": hyperparameters[\"env_id\"],\n",
    "          \"mean_reward\": mean_reward,\n",
    "          \"n_evaluation_episodes\": hyperparameters[\"n_evaluation_episodes\"],\n",
    "          \"eval_datetime\": eval_form_datetime,\n",
    "    }\n",
    "\n",
    "    # Write a JSON file\n",
    "    with open(local_directory / \"results.json\", \"w\") as outfile:\n",
    "        json.dump(evaluate_data, outfile)\n",
    "\n",
    "    # Step 5: Create the model card\n",
    "    env_name = hyperparameters[\"env_id\"]\n",
    "\n",
    "    metadata = {}\n",
    "    metadata[\"tags\"] = [\n",
    "          env_name,\n",
    "          \"reinforce\",\n",
    "          \"reinforcement-learning\",\n",
    "          \"custom-implementation\",\n",
    "          \"deep-rl-class\"\n",
    "      ]\n",
    "\n",
    "    # Add metrics\n",
    "    eval = metadata_eval_result(\n",
    "        model_pretty_name=repo_name,\n",
    "        task_pretty_name=\"reinforcement-learning\",\n",
    "        task_id=\"reinforcement-learning\",\n",
    "        metrics_pretty_name=\"mean_reward\",\n",
    "        metrics_id=\"mean_reward\",\n",
    "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
    "        dataset_pretty_name=env_name,\n",
    "        dataset_id=env_name,\n",
    "      )\n",
    "\n",
    "    # Merges both dictionaries\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    model_card = f\"\"\"\n",
    "  # **Reinforce** Agent playing **{env_id}**\n",
    "  This is a trained model of a **Reinforce** agent playing **{env_id}** .\n",
    "  To learn to use this model and train yours check Unit 4 of the Deep Reinforcement Learning Course: https://huggingface.co/deep-rl-course/unit4/introduction\n",
    "  \"\"\"\n",
    "\n",
    "    readme_path = local_directory / \"README.md\"\n",
    "    readme = \"\"\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
    "          readme = f.read()\n",
    "    else:\n",
    "      readme = model_card\n",
    "\n",
    "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "      f.write(readme)\n",
    "\n",
    "    # Save our metrics to Readme metadata\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "    # Step 6: Record a video\n",
    "    video_path =  local_directory / \"replay.mp4\"\n",
    "    record_video(env, model, video_path, video_fps)\n",
    "\n",
    "    # Step 7. Push everything to the Hub\n",
    "    api.upload_folder(\n",
    "          repo_id=repo_id,\n",
    "          folder_path=local_directory,\n",
    "          path_in_repo=\".\",\n",
    "    )\n",
    "\n",
    "    print(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJMhj_5cLNVk"
   },
   "source": [
    "# TODO\n",
    "1. check performance\n",
    "2. check execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "DkNCmK5cBdIr",
    "outputId": "3c16fc4a-e53e-42ec-a901-8befda01a0c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liqiu/opt/anaconda3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/liqiu/opt/anaconda3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/liqiu/opt/anaconda3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/liqiu/opt/anaconda3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/Users/liqiu/opt/anaconda3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      "/Users/liqiu/opt/anaconda3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/liqiu/opt/anaconda3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/var/folders/_t/z3tjm9014td81ypk6h_0423c0000gn/T/ipykernel_1361/2540619266.py:186: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1711403209812/work/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  logps = get_logp(policy = policy, obs= torch.as_tensor(batch_st), acts = torch.as_tensor(batch_at))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th iteration, p loss=-0.5539915035501888, v_loss=4.323989354147111 rewards=-4.94, std=0.2374868417407583 lens=5.9\n",
      "\n",
      "10th iteration, p loss=0.013744567513046107, v_loss=1.0579919229158234 rewards=-2.01, std=1.41063815346105 lens=24.66\n",
      "\n",
      "20th iteration, p loss=0.007731903371721825, v_loss=1.1494467297262074 rewards=-2.42, std=0.7769169839822013 lens=22.37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vanila policy gradient descent with reward to go and BASELINE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import gym_pygame\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# hyperparamebers to tune\n",
    "learning_rate =  1e-3\n",
    "gamma = 0.9\n",
    "\n",
    "debug = False\n",
    "\n",
    "# File path to the model\n",
    "policy_model_path = \"policy_model.pth\"\n",
    "value_model_path = \"value_model.pth\"\n",
    "video_fps = 30\n",
    "# video_path = 'replay.mp4'\n",
    "\n",
    "\n",
    "def load_model(model, model_file_path):\n",
    "  # Check if the model file exists\n",
    "    if os.path.exists(model_file_path):\n",
    "        print(f\"Loading model from {model_file_path}...\")\n",
    "        # Load the saved model state\n",
    "        model.load_state_dict(torch.load(model_file_path))\n",
    "    else:\n",
    "        print(f\"No saved model found at {model_file_path}. Creating a new model...\")\n",
    "        \n",
    "def save_model(model, model_file_path):\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), model_file_path)\n",
    "\n",
    "def disable_random():\n",
    "    # disable all random ness\n",
    "    # Set the random seed for Python's random module\n",
    "    random.seed(0)\n",
    "    # Set the random seed for NumPy (if you're using it)\n",
    "    np.random.seed(0)\n",
    "    # Set the random seed for PyTorch (CPU and GPU)\n",
    "    torch.manual_seed(0)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(0)\n",
    "\n",
    "def print_trajectories(batch_traj):\n",
    "    #debug if action is random enough\n",
    "    if debug:\n",
    "        actions = []\n",
    "        for st, at, reward, st_1, done in batch_traj:\n",
    "            actions.append(at)\n",
    "        print(f'actions: {actions}')\n",
    "\n",
    "def print_rewards(traj_rewards):\n",
    "    #debug if the random b actually yield difference in rewards\n",
    "    if debug:\n",
    "        print(f'traj rewards: {traj_rewards}')\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    # (TODO) MLP/Policy can be reconfigured to contain multiple layers\n",
    "    def __init__(self, dim_input, dim_hidden, dim_output):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim_input, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, 2 * dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * dim_hidden, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, dim_output)\n",
    "        )\n",
    "    # return logits of size(dim_output)\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "class Value(nn.Module):\n",
    "    # (TODO) MLP/Policy can be reconfigured to contain multiple layers\n",
    "    def __init__(self, dim_input, dim_hidden):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim_input, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, 2 * dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * dim_hidden, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "# action(st), H(st) -> at based on probability distribution\n",
    "def take_action(policy, s):\n",
    "    logits = policy(s)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    categorical_dist = torch.distributions.Categorical(probs = probs)\n",
    "    action = categorical_dist.sample()\n",
    "    return action.item()\n",
    "\n",
    "def get_logp(policy, obs, acts):\n",
    "    logits = policy(obs)\n",
    "    categorical_dists = torch.distributions.Categorical(logits = logits)\n",
    "    return categorical_dists.log_prob(acts)\n",
    "\n",
    "def get_values(value, obs):\n",
    "    baselines = value(obs)\n",
    "    return baselines\n",
    "\n",
    "    \n",
    "# return loss, avg_reward, avg_lens for this epoch\n",
    "def train_epoch(env, policy, value, num_batch, policy_opt, value_opt):\n",
    "    # return [(st, at, reward, st+1, done)]\n",
    "    def generate_trajectory(env, policy, num_batch):\n",
    "        batch_traj = []\n",
    "        for i in range(num_batch):\n",
    "            st = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = take_action(policy, torch.tensor(st))\n",
    "                st_1, reward, done, _ = env.step(action)\n",
    "                batch_traj.append((st, action, reward, st_1, done))\n",
    "                st = st_1\n",
    "        return batch_traj\n",
    "    \n",
    "    # return reward to go\n",
    "    def compute_reward_to_go(batch_traj):\n",
    "        rtgs = []\n",
    "        curr_traj = []\n",
    "        for st, at, reward, st_1, done in batch_traj:\n",
    "            curr_traj.append((st, at, reward, st_1, done))\n",
    "            if done:\n",
    "                # append rewards to gos from current trajectory to rtgs\n",
    "                rtgs.extend(get_rtgs_from_current_traj(curr_traj))\n",
    "                curr_traj = []\n",
    "        return rtgs\n",
    "                \n",
    "    def get_rtgs_from_current_traj(curr_traj):\n",
    "        n = len(curr_traj)\n",
    "        rtgs = [0] * n\n",
    "        for i in reversed(range(n)):\n",
    "            st, at, reward, st_1, done = curr_traj[i]\n",
    "            if i == n - 1:\n",
    "                rtgs[i] = reward\n",
    "            else:\n",
    "                rtgs[i] = reward + gamma * rtgs[i + 1]\n",
    "        return rtgs \n",
    "    \n",
    "    # traj: [(st, at, reward, st+1, done)]\n",
    "    def fit_value_evaluation(value, value_opt, batch_traj):\n",
    "        predications = []\n",
    "        targets = []\n",
    "        for st, at, reward, st_1, done in batch_traj:\n",
    "            v_st = value(torch.as_tensor(st))\n",
    "            v_st_1 = value(torch.as_tensor(st_1))\n",
    "            predications.append(v_st)\n",
    "            targets.append(reward + gamma * v_st_1)\n",
    "        value_opt.zero_grad()\n",
    "        targets = torch.as_tensor(targets, dtype=torch.float64)\n",
    "        predications = torch.stack(predications).squeeze(-1)\n",
    "        loss = nn.MSELoss()(predications, targets)\n",
    "        loss.backward()\n",
    "        value_opt.step()\n",
    "        return loss.item()\n",
    "        \n",
    "    \n",
    "    def fit_policy_evaluation(policy, policy_opt, batch_traj, value):\n",
    "        batch_adv = [] # [tensor(torch.double)]\n",
    "        batch_st = []\n",
    "        batch_at = []\n",
    "        # compute adv(st, at)\n",
    "        with torch.no_grad():\n",
    "            for st, at, reward, st_1, done in batch_traj:\n",
    "                v_st = value(torch.as_tensor(st)).item()\n",
    "                v_st_1 = value(torch.as_tensor(st_1)).item()\n",
    "                batch_adv.append(reward + gamma * v_st_1 - v_st)\n",
    "                batch_st.append(st)\n",
    "                batch_at.append(at)\n",
    "        batch_adv = torch.as_tensor(batch_adv)\n",
    "        logps = get_logp(policy = policy, obs= torch.as_tensor(batch_st), acts = torch.as_tensor(batch_at))\n",
    "        loss = -(logps * batch_adv).mean()\n",
    "        policy_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        policy_opt.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    # return mean(reward), len(reward), std(reward)\n",
    "    def evaluate_policy(batch_traj):\n",
    "        sampled_rewards = []\n",
    "        traj_lens = []\n",
    "        traj_reward = 0\n",
    "        len = 0\n",
    "        for st, at, reward, st_1, done in batch_traj:\n",
    "            len += 1\n",
    "            traj_reward += reward\n",
    "            if done:\n",
    "                sampled_rewards.append(traj_reward)\n",
    "                traj_lens.append(len)\n",
    "                traj_reward = 0\n",
    "                len = 0\n",
    "        return np.mean(sampled_rewards), np.std(sampled_rewards), np.mean(traj_lens)\n",
    "                \n",
    "        \n",
    "    # start = timer()\n",
    "    batch_traj = generate_trajectory(env, policy, num_batch)\n",
    "    # batch_rtgs = compute_reward_to_go(batch_traj)\n",
    "    print_trajectories(batch_traj)\n",
    "    # print_rewards(batch_rtgs)\n",
    "    v_loss = fit_value_evaluation(value = value, value_opt=value_opt, batch_traj=batch_traj)\n",
    "    p_loss = fit_policy_evaluation(policy = policy, policy_opt = policy_opt, batch_traj = batch_traj, value = value)\n",
    "    reward_mean, reward_std, len_mean = evaluate_policy(batch_traj)\n",
    "    \n",
    "    return p_loss, v_loss, reward_mean, len_mean, reward_std\n",
    "def train():\n",
    "    start = timer()\n",
    "    #debug\n",
    "    # disable_random(\n",
    "    env = gym.make(\"Pixelcopter-PLE-v0\")\n",
    "    # make the action deterministic\n",
    "    dim_input = env.observation_space.shape[0]\n",
    "    dim_output = env.action_space.n\n",
    "    policy = Policy(dim_input, 32, dim_output).double()\n",
    "    value = Value(dim_input, 32).double()\n",
    "    # load_model(policy, policy_model_path)\n",
    "    # load_model(value, value_model_path)\n",
    "    policy_opt = torch.optim.Adam(policy.parameters(), lr = learning_rate)\n",
    "    value_opt = torch.optim.Adam(value.parameters(), lr = learning_rate)\n",
    "\n",
    "    num_epoch = 50\n",
    "    num_batch = 100\n",
    "    rewards = []\n",
    "    reward_std = []\n",
    "    for i in range(num_epoch):\n",
    "        p_loss, v_loss, reward, lens, reward_st = train_epoch(env, policy, value, num_batch, policy_opt, value_opt)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"{i}th iteration, p loss={p_loss}, v_loss={v_loss} rewards={reward}, std={reward_st} lens={lens}\\n\")\n",
    "        # print(f\"{i}th iteration, loss={loss}, rewards={reward}, std={reward_st} lens={lens}\\n\")\n",
    "\n",
    "        rewards.append(reward)\n",
    "        reward_std.append(reward_st)\n",
    "    print(f'total time for traning: {timer() - start}')\n",
    "    plt.plot(list(range(num_epoch)), rewards, marker='o', linestyle='-', label='Rewards per Epoch')\n",
    "    plt.plot(list(range(num_epoch)), reward_std, marker='s', linestyle='--',  color='red', label='std per Epoch')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.title('Rewards per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    save_model(policy, policy_model_path)\n",
    "    save_model(value, value_model_path)\n",
    "    policy.eval()\n",
    "    current_time = datetime.now()\n",
    "\n",
    "    # Format the date and time as mm-dd-tt\n",
    "    formatted_time = current_time.strftime(\"%m-%d-%H:%M\")  # %H%M represents hours and minutes (24-hour format)\n",
    "\n",
    "    # Append the formatted time to the string 'replay'\n",
    "    video_path = f\"replay-{formatted_time}.mp4\"\n",
    "    # record_video(env, policy, video_path, video_fps)\n",
    "\n",
    "train()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Conda Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
